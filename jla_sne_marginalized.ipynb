{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.interpolate as interpolate\n",
    "import simulators.jla_supernovae.jla as jla\n",
    "import simulators.jla_supernovae.jla_parser as jla_parser\n",
    "import ndes.nde as nde\n",
    "import distributions.priors as priors\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jalsing/Dropbox (Simons Foundation)/science/delfi/DELFI_MASTER/delfi/simulators/jla_supernovae/jla_parser.py:9: VisibleDeprecationWarning: Reading unicode strings without specifying the encoding argument is deprecated. Set the encoding, use None for the system default.\n",
      "  dtype = None, names = True)\n"
     ]
    }
   ],
   "source": [
    "### SET UP FOR SIMULATION CODE ###\n",
    "\n",
    "# Import data\n",
    "jla_data, jla_cmats = jla_parser.b14_parse(z_min=None, z_max=None, qual_cut=False,\n",
    "                                    jla_path='simulators/jla_supernovae/jla_data/')\n",
    "data = jla_data['mb']\n",
    "delta_m_cut = 10\n",
    "auxiliary_data = np.column_stack([jla_data['zcmb'], jla_data['x1'], jla_data['color'], np.array([(jla_data['3rdvar'] > delta_m_cut)], dtype=int)[0]])\n",
    "\n",
    "# Om, w0, M_b, alpha, beta, delta_m\n",
    "npar = 6\n",
    "theta_fiducial = np.array([  0.20181324,  -0.74762939, -19.04253368,   0.12566322,   2.64387045, -0.05252869])\n",
    "\n",
    "# Define prior limits and boundaries\n",
    "lower = np.array([0, -1.5, -20, 0, 0, -0.5])\n",
    "upper = np.array([0.6, 0, -18, 1, 6, 0.5])\n",
    "Q = np.diag([0.4, 0.75, 0.1, 0.025, 0.25, 0.05])**2\n",
    "Q[0,1] = Q[1,0] = -0.8*0.4*0.75\n",
    "Qinv = np.linalg.inv(Q)\n",
    "prior_mean = np.array([  0.3  ,  -0.75 , -19.05 ,   0.125,   2.6  ,  -0.05 ])\n",
    "prior_args = [prior_mean, Q, lower, upper]\n",
    "\n",
    "# Covariance matrix\n",
    "C = jla_parser.b14_covariance(jla_data, jla_cmats, theta_fiducial[3], theta_fiducial[4])\n",
    "Cinv = np.linalg.inv(C)\n",
    "L = np.linalg.cholesky(C)\n",
    "\n",
    "# Derivative of the covariance matrix\n",
    "n_sn = len(C)\n",
    "dCdt = np.zeros((npar, n_sn, n_sn))\n",
    "\n",
    "# Step size for derivatives\n",
    "step = abs(0.01*theta_fiducial)\n",
    "\n",
    "# N data points\n",
    "ndata = len(jla_data['mb'])\n",
    "\n",
    "# Simulation args\n",
    "sim_args = [auxiliary_data, L]\n",
    "\n",
    "# Compute the mean\n",
    "mu = jla.apparent_magnitude(theta_fiducial, auxiliary_data)\n",
    "\n",
    "# Compute the derivatives\n",
    "dmdt = jla.dmudtheta(theta_fiducial, jla.simulation_seeded, step, npar, ndata, sim_args)\n",
    "dmdt[2,:] = np.ones(n_sn)\n",
    "dmdt[3,:] = -jla_data['x1']\n",
    "dmdt[4,:] = jla_data['color']\n",
    "dmdt[5,:] = (jla_data['3rdvar'] > 10)\n",
    "\n",
    "# Fisher matrix\n",
    "F, Finv = jla.fisher(dmdt, dCdt, Cinv, Qinv, npar)\n",
    "fisher_errors = np.sqrt(np.diag(Finv))\n",
    "\n",
    "# Compute projection vectors\n",
    "Fpinv = np.linalg.inv(F[2:,2:])\n",
    "P1 = np.dot(Fpinv, F[0,2:])\n",
    "P2 = np.dot(Fpinv, F[1,2:])\n",
    "\n",
    "# Simulation args for ABC\n",
    "compressor_args = [theta_fiducial, Finv, Cinv, dmdt, dCdt, mu, Qinv, prior_mean, F, P1, P2]\n",
    "\n",
    "# Parameter names for plotting\n",
    "names = ['\\Omega_m', 'w_0', 'M_\\mathrm{B}', '\\alpha', '\\beta', '\\delta M']\n",
    "labels =  ['\\\\Omega_m', 'w_0', 'M_\\mathrm{B}', '\\\\alpha', '\\\\beta', '\\\\delta M']\n",
    "ranges = {'\\Omega_m':[lower[0], upper[0]], '\\w0':[lower[1], upper[1]]}\n",
    "\n",
    "# Compressed dataset\n",
    "data = jla.compressor_projected(data, compressor_args)\n",
    "\n",
    "# Define new separate priors over eta and theta\n",
    "theta_fiducial = np.array([0.20181324,  -0.74762939])\n",
    "Finv = Finv[0:2,0:2]\n",
    "lower = np.array([0, -1.5])\n",
    "upper = np.array([0.6, 0])\n",
    "Q = np.diag([0.4, 0.75])**2\n",
    "Q[0,1] = Q[1,0] = -0.8*0.4*0.75\n",
    "Qinv = np.linalg.inv(Q)\n",
    "prior_mean = np.array([  0.3  ,  -0.75])\n",
    "prior = priors.TruncatedGaussian(prior_mean, Q, lower, upper)\n",
    "\n",
    "eta_prior = priors.TruncatedGaussian(np.array([-19.05, 0.125, 2.6, -0.05]), \n",
    "                                     np.diag([0.1, 0.025, 0.25, 0.05])**2, \n",
    "                                     np.array([-20, 0, 0, -0.5]),\n",
    "                                     np.array([-18, 1, 6, 0.5]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the simulator function: takes parameters, spits out simulated data\n",
    "# Should have the form: simulator(parameters, args) -> simulated dataset\n",
    "def simulator(theta, simulator_args):\n",
    "    \n",
    "    eta = eta_prior.draw()\n",
    "    return jla.simulation(np.concatenate([theta, eta]), simulator_args)\n",
    "simulator_args = sim_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the compression function: takes data, spits out compressed summaries\n",
    "# Should have the form compressor(data, args) -> compressed summaries\n",
    "# NB: compression should be set-up like a quasi maximum-likelihood estimator\n",
    "compressor = jla.compressor_projected\n",
    "compressor_args = compressor_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DELFI MDN object\n",
    "n_components = 3\n",
    "\n",
    "mdn = nde.DelfiMixtureDensityNetwork(data, prior, [lower, upper], Finv, theta_fiducial, n_components, n_hidden = [50, 50], activations = ['tanh', 'tanh'], names = names, labels = labels, ranges = ranges, results_dir='simulators/jla_supernovae/results_marginalized/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating fisher pre-training data...\n",
      "Training on the pre-training data...\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/100\n",
      "44900/45000 [============================>.] - ETA: 0s - loss: 1.9296"
     ]
    }
   ],
   "source": [
    "# Do the Fisher pre-training\n",
    "mdn.fisher_pretraining(50000, prior, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proposal for the SNL\n",
    "proposal = priors.TruncatedGaussian(theta_fiducial, 9*Finv, lower, upper)\n",
    "\n",
    "# Initial samples, batch size for population samples, number of populations\n",
    "n_initial = 100\n",
    "n_batch = 100\n",
    "n_populations = 99\n",
    "\n",
    "# Do the SNL training\n",
    "mdn.sequential_training(simulator, compressor, n_initial, n_batch, n_populations, proposal, simulator_args=simulator_args, compressor_args=compressor_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trace plot of the loss as a function of the number of simulations\n",
    "plt.scatter(mdn.n_sim_trace, mdn.loss_trace, s = 20)\n",
    "plt.plot(mdn.n_sim_trace, mdn.loss_trace, color = 'red')\n",
    "plt.xlim(0, mdn.n_sim_trace[-1])\n",
    "plt.xlabel('number of simulations')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
